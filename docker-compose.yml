services:
  gradio_app:
    build: ./gradio_app
    ports:
      - "7860:7860"
    volumes:
      - ./gradio_app:/app
    environment:
      # For S3
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}

      # URL for the new LLM service (FastAPI)
      - LLM_SERVICE_URL=http://llm_service:8000
      # URL for the Text-to-Image service
      - TEXT_TO_IMAGE_SERVICE_URL=http://text_to_image_service:8001
      # URL for the 3D Generation service (FastAPI)
      - THREED_GENERATION_SERVICE_URL=http://threed_generation_service:8002
    depends_on:
      - llm_service
      - text_to_image_service
      - threed_generation_service
    networks:
      - app_network

  llm_service:
    build: ./llm_service
    ports:
      - "8000:8000"
    networks:
      - app_network
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY} # Injected from .env
    volumes:
      - ./llm_service:/app # Mount the llm_service code
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload

  text_to_image_service: # New service for text-to-image
    build:
      context: ./text_to_image_service
      # platform: linux/amd64 # Keep this if you are on an M1/M2 Mac and the image needs to be amd64
    ports:
      - "8001:8001"
    networks:
      - app_network
    volumes:
      - ./text_to_image_service:/app # Mount the text_to_image_service code
      - huggingface_cache_image_gen:/home/appuser_image_gen/.cache/huggingface # Named volume for Hugging Face cache
    command: uvicorn main:app --host 0.0.0.0 --port 8001 --reload

  threed_generation_service: # New service for 3D model generation
    build: ./threed_generation_service
    ports:
      - "8002:8002" # Expose a port for this service
    networks:
      - app_network
    environment:
      - SYNEXA_API_KEY=${SYNEXA_API_KEY} # Will be read from .env file
    volumes:
      - ./threed_generation_service:/app # Mount the service code
      # - threed_models_cache:/app/models # Optional: if you want to cache models downloaded by the service itself
    command: uvicorn main:app --host 0.0.0.0 --port 8002 --reload
    # platform: linux/amd64 # Add if building on ARM and deploying to AMD64 or vice-versa for pytorch compatibility

networks:
  app_network:
    driver: bridge

volumes:
  huggingface_cache_image_gen: # Define the named volume used by text_to_image_service
